# What Was Actually Running — and Why It Matters

Before discussing the regression documented in this repository, it is essential to understand **what was actually running on this system**.

Not in abstract terms.  
Not as a theoretical workload.  
But as proven, end-to-end computation reflected directly in the artifacts you can inspect yourself.

This section answers a single question:

> **What exactly was running on this system, how demanding was it, and why would it stress code paths that most users never touch?**

---

## 1. Ground Truth: What the Artifacts Themselves Prove

The following facts are directly verifiable from the output files included in this repository.  
They are **observations**, not interpretations or assumptions.

### Verified Properties of the Outputs

- All outputs are **full-resolution JPG files**
- They are **final decoded images**, not previews or latent visualizations
- The model class used is **SDXL or SDXL-derived checkpoints**
- **SDXL Refiner** was enabled
- **Seeds were locked** for every generation
- The **Karras (2M) scheduler** was used
- **CFG (guidance scale) = 6.8**
- **38 denoising steps per image**
- Generations completed **successfully**, repeatedly, across a **single extended session**

---

## 2. Representative Image Sizes and Memory Footprint

The table below shows a **representative subset** of images from the session.  
These are not cherry-picked; they illustrate the resolution range and formats used throughout the run.

| File                     | Resolution   | MPix | JPG Size | Decoded RGB (≈) |
|--------------------------|--------------|------|----------|-----------------|
| TempestV0.1-Artistic     | 1152×1536    | 1.77 | 247 KB   | 5.06 MB         |
| tempestByVlad_base       | 1152×1536    | 1.77 | 376 KB   | 5.06 MB         |
| tempestByVlad_base       | 1024×1024    | 1.05 | 142 KB   | 3.00 MB         |
| sd_xl_base_1.0           | 1024×1024    | 1.05 | 178 KB   | 3.00 MB         |
| tempestByVlad_base       | 1152×1152    | 1.33 | 190 KB   | 3.80 MB         |
| tempestByVlad_base       | 848×1152     | 0.98 | 249 KB   | 2.79 MB         |
| TempestV0.1-Artistic     | 1024×1536    | 1.57 | 298 KB   | 4.50 MB         |

### Session-Level Scale (Authoritative)

- **500 final images generated**
- **≈ 13.5 MPix per ~10 images → ≈ 676 MPix total**
- **≈ 115–120 MB total on disk** (compressed JPG)
- **≈ 1.9–2.0 GB decoded RGB data** when viewed, composited, or cached

> This last number matters: the **desktop graphics pipeline failed**, not inference itself.

---

## 3. SDXL Latent-Space Reality (The Actual Scale of the Work)

SDXL operates in latent space at **1/8 resolution**.

All images in this session divide cleanly by 8, so no ambiguity or padding occurs at the VAE boundary.

### Representative Latent Dimensions

- `1152×1536 → 144×192 → 27,648 latent positions`
- `1024×1024 → 128×128 → 16,384 positions`
- `1024×1536 → 128×192 → 24,576 positions`
- `848×1152 → 106×144 → 15,264 positions`

From the representative set:

- **≈ 21,149 latent positions per image (average)**

### Session-Level Latent Grid

≈ 21,149 latent positions × 500 images
≈ 10,574,500 latent positions


This is the spatial grid that **every UNet block touches repeatedly**, for **every denoising step**, for **both base and refiner passes**.

---

## 4. Why Steps, CFG, and the Refiner Matter

### 4.1 38 Denoising Steps (Base + Refiner)

At **38 steps**, total latent updates are:

10,574,500 latent positions × 38 steps
≈ 401,831,000 latent-position updates


This excludes:

- Multiple convolution layers per step  
- Multiple attention blocks per step  
- The **SDXL Refiner**, which reprocesses high-frequency detail using a second UNet pass  

The refiner does **not** double compute linearly, but it **extends activation lifetimes**, re-maps buffers, and re-enters CUDA graph paths.

---

### 4.2 CFG = 6.8 (Guidance Is Not Free)

Classifier-free guidance is **not** a scalar multiplier. It requires:

- Parallel unconditional and conditional passes  
- Blending operations at every step  
- Extended activation lifetimes  

At **CFG 6.8**, guidance is firmly in the **high-influence regime**, increasing:

- Intermediate tensor retention  
- Synchronization points  
- Memory allocator churn  

---

## 5. Cross-Attention Pressure with Long Prompts

Prompt lengths in this session exceeded **~2400 tokens**.

Cross-attention complexity scales as:

latent_positions × token_count


### Per-Image Example (Typical)

≈ 21,149 × 2400
≈ 50,757,600 attention elements


If naïvely materialized in fp16:

- **≈ 96.8 MB per attention matrix**
- **Per attention block**
- **Per denoising step**

### Session-Level Attention Pressure

Per step, across 500 images:

≈ 25.4 billion attention elements
≈ ~48 GB fp16-equivalent memory traffic per step


Across **38 steps**:

≈ 965 billion attention elements
≈ ~1.8 TB fp16-equivalent memory traffic


> This is **not VRAM allocated at once**.  
> It is a **conservative, defensible proxy** for allocator pressure, mapping churn, kernel residency time, and driver bookkeeping stress.

---

## 6. Session Dynamics That Matter

This workload combines stressors that most users never stack together:

- **500 images in a single extended session**
- **Long prompts (~2400 tokens)**
- **38 steps with CFG 6.8**
- **SDXL Refiner enabled**
- **Locked seeds causing deterministic reuse of identical memory paths**

Executed inside **SD.Next**, which maintains:

- Long-lived CUDA contexts  
- Dynamic pipeline reconfiguration  
- Model reloads and graph recompilation  

This produces sustained pressure on:

- CUDA memory allocators  
- GPU virtual address mapping (including BAR1)  
- Driver-level state shared with the desktop compositor  

---

## 7. Why Success Is the Key Signal

The defining signal in this case is **not inference failure**.

The workload:

- Completed successfully  
- Produced high-fidelity outputs  
- Raised no inference-time errors  

Instability appeared **only after returning to routine desktop operations**, such as:

- Screenshot capture  
- Image viewing  
- Desktop composition  

This pattern indicates:

- Kernels executed correctly  
- Allocations succeeded  
- The regression surfaced **below the application layer**  
- Within **shared OS and driver runtime paths**, after sustained stress  

That is the signature of a **post-workload runtime regression**, not misuse, misconfiguration, or insufficient hardware.

---

## Executive Summary 

> This session executed approximately **500 SDXL image generations** using **38-step Karras scheduling**, **CFG 6.8**, **long-token prompts**, **locked seeds**, and the **SDXL Refiner**, producing **hundreds of millions of latent updates and nearly a trillion attention-element interactions** across extended CUDA contexts — a workload sufficient to stress allocator, mapping, and compositor paths far beyond typical desktop usage, even though inference itself completed successfully.
